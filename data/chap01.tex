% !TeX root = ../thuthesis-example.tex
\thusetup{
  cite-style = inline,
}

\chapter{课题背景与意义}

% 研究生学位论文撰写，除表达形式上需要符合一定的格式要求外，内容方面上也要遵循一些共性原则。

% 通常研究生学位论文只能有一个主题（不能是几块工作拼凑在一起），该主题应针对某学科领域中的一个具体问题展开深入、系统的研究，并得出有价值的研究结论。
% 学位论文的研究主题切忌过大，例如，“中国国有企业改制问题研究”这样的研究主题过大，因为“国企改制”涉及的问题范围太广，很难在一本研究生学位论文中完全研究透彻。



% \section{研究背景}

% 具身移动操作（Embodied Mobile Manipulation）的核心目标是在真实或高保真拟真环境中实现“感知—理解—规划—控制”的闭环，
% 使智能体能够依据自然语言或任务指令自主完成各种操作任务。这通常需要机器人具备完备的环境感知与理解，并根据任务类型和操作空间范围实现自主移动与操作规划。
% 在面向实际通用场景（如家庭、实验室与仓储）中，操作任务并非单一的导航或单一的操作，而是先在未知或半未知空间中移动到合理的语义上下文（例如工作台、目标设备或特定区域），继而完成各种如抓取、开关、插拔、放置等精细动作。
% 因此，面向通用的具身移动操作，更要求机器人在各种复杂场景下，实现稳定的任务执行，并具备跨任务与跨环境的泛化能力、对感知噪声与动态扰动的不敏感与鲁棒性，
% 以及在语义指令驱动下将导航与精细操作一体化闭环（如主动感知、约束安全与失败恢复）的能力。

% 近年来，基于通用模型(Foundation Model)驱动的具身移动和操作分别取得了有效的进展。
% 在移动方面，视觉-语言-导航(Visual-Language-Navigation, VLN)在从语言到可达目标的路径规划方面实现了丰富的积累，在仿真和真机中呈现出令人印象深刻的效果；
% 同时，无论是基于模仿学习(Imitation Learning, IL)还是离线强化学习(Offline Reinforcement Learning, RL)的操作方法，在对象识别、姿态估计、抓取规划等方面都取得了显著的进展，并在真实机器人平台上得到了验证。
% 然而，在移动操作任务中，移动和操作之间存在不可忽视的强耦合：
% 移动阶段所形成的视角、距离与遮挡关系直接决定了后续操作的观测质量与可达性，
% 而操作对目标物体、姿态与约束的先验又会反过来影响移动的策略选择与路径代价。
% 因此，仅在导航或操作单一子任务上取得进展，并不足以保证端到端任务的成功率、稳定性与泛化能力。

% 与此同时，以视觉-语言-动作（Visual-Language-Action, VLA）为代表的大模型在指令理解、情景推理与跨任务迁移上展现出统一表达与泛化能力，
% 通过将开放式语言意图与感知结果对齐为可执行中间表示并生成带约束的分层技能序列，从而支撑“从语言到动作”的一体化规划与在动态噪声下的鲁棒重规划，
% 为“从语言到可执行子目标与技能序列”的一体化规划提供了新的可行性。
% 因此，如何将大模型的语义优势落地为低层可控、可验证的移动与操作策略，并在扰动与不确定条件下保持鲁棒，是当前研究的重要方向。


% \section{课题来源}

% 近年来，随着服务机器人、实验室助理机器人等应用的推进，机器人在室内环境中“既能自主移动，又能完成抓取、放置、开关等操作”的需求逐渐增加。
% 无论是在家居、办公室，还是在实验室等典型室内场景中，机器人往往需要先在环境中移动到合适的位置，再对目标物体或设备执行相应操作，
% 这类“具身移动操作”任务逐渐成为智能机器人研究中的一个重要方向。
% 与此同时，多模态基础模型和视觉-语言-动作大模型的发展，使得机器人有可能在同一模型框架下处理视觉、语言与动作信息，
% 从自然语言指令出发完成环境理解和行为生成，这为改进现有移动操作方法提供了新的思路。

% 然而，现有很多工作要么只关注固定机械臂的操作，要么只针对局部、小范围的导航任务；即便有部分移动操作系统可以完成端到端任务，
% 其方法多半针对特定场景或特定任务进行定制，对视觉、语言与动作的统一建模和可扩展性仍然有限。
% 在这种背景下，围绕“基于视觉-语言-动作大模型的具身移动操作方法”开展研究，一方面可以呼应当前多模态大模型在机器人领域落地的趋势，
% 另一方面也有助于提升机器人在典型通用场景中的任务执行能力。

% 本课题“面向通用场景的基于视觉-语言-动作大模型的具身移动操作方法研究”主要面向各类通用场景（如家庭、实验室与仓储等）中具有代表性的移动操作任务，
% 重点关注如何利用视觉-语言-动作大模型统一处理指令理解、环境感知和移动/操作决策问题。课题的提出一方面源于当前具身智能与大模型结合的研究动向，
% 另一方面也基于前期在视觉-语言导航（VLN）和具身操作方面已有的算法和系统积累，
% 希望在此基础上进一步探索一种具有一定通用性、但仍然可控可实现的具身移动操作方法。


% \section{研究意义}

% 从学术研究角度看，围绕“面向通用场景的基于视觉-语言-动作大模型的具身移动操作方法”开展系统研究，有望在以下几个方面形成有价值的探索：
% 一是通过引入统一的中间表示，将自然语言指令、环境语义信息与可执行动作在同一框架下对齐，为移动与操作的协同规划提供更清晰的语义—几何映射途径；
% 二是基于视觉-语言-动作大模型，将指令理解、情景推理与技能序列生成结合起来，探索从“语言—感知—动作”的端到端一体化方法，为具身智能中高层语义推理与低层控制之间的连接提供新的思路；
% 三是面向典型通用场景，系统分析移动与操作强耦合带来的问题，设计相应的鲁棒策略与评测指标，为今后更大规模、更复杂场景下的具身移动操作研究打下方法和系统基础。

% 从应用需求角度看，本课题的研究有望为实验室助理机器人、物流仓储机器人乃至面向家居服务机器人的具身移动操作任务提供可落地的技术方案。
% 通过提升机器人在通用场景中的整体能力，可以在一定程度上减少对场景的人工改造与精细规则设计，降低系统部署和维护成本。
% 同时，得益于视觉-语言-动作大模型的统一建模能力，所提出的方法在后续扩展到新任务、新物体或新环境时，具有更好的可迁移性和可扩展性。



\section{研究背景}

近年来，随着人工智能基础模型（Foundation Models）和多模态大模型（Multi-modal Large Models）的快速发展，具身智能（Embodied Intelligence）逐渐被视为迈向通用人工智能的重要路径之一，
其核心目标是在统一的“感知—理解—决策—执行”闭环中，使智能体能够在真实或高保真拟真环境中通过与物理世界的持续交互获取知识、完成复杂任务
\cite{Liu2025AligningCyberSpace,Xiao2025Robot}。
在这一大背景下，具身移动操作（Embodied Mobile Manipulation）成为连接认知智能与物理执行能力的关键形态之一：
机器人不仅需要具备在三维空间中自主移动的能力，还需在语义丰富、结构复杂的环境中完成抓取、放置、开关、插拔等多样化操作，从而实现从“可导航”到“可服务”的跨越
\cite{Thakar2023Survey}。
与传统仅关注机械臂操作或仅关注移动平台导航的研究不同，具身移动操作强调在统一任务视角下对移动与操作进行协同建模和联合优化，即在语言或任务指令驱动下，
依据环境语义、物体状态和任务约束进行一体化的运动规划和执行。

近年来，基于通用模型(Foundation Model)驱动的具身移动和操作分别取得了有效的进展。
在移动方面，视觉-语言-导航(Visual-Language-Navigation, VLN)在从语言到可达目标的路径规划方面实现了丰富的积累，在仿真和真机中呈现出令人印象深刻的效果；
同时，无论是基于模仿学习(Imitation Learning, IL)还是离线强化学习(Offline Reinforcement Learning, RL)的操作方法，在对象识别、姿态估计、抓取规划等方面都取得了显著的进展，并在真实机器人平台上得到了验证。
然而，在移动操作任务中，移动和操作之间存在不可忽视的强耦合：
移动阶段所形成的视角、距离与遮挡关系直接决定了后续操作的观测质量与可达性，
而操作对目标物体、姿态与约束的先验又会反过来影响移动的策略选择与路径代价。
因此，仅在导航或操作单一子任务上取得进展，并不足以保证端到端任务的成功率、稳定性与泛化能力。
与此同时，以视觉-语言-动作（Visual-Language-Action, VLA）为代表的大模型在指令理解、情景推理与跨任务迁移上展现出统一表达与泛化能力，
通过将开放式语言意图与感知结果对齐为可执行中间表示并生成带约束的分层技能序列，从而支撑“从语言到动作”的一体化规划与在动态噪声下的鲁棒重规划，
为“从语言到可执行子目标与技能序列”的一体化规划提供了新的可行性。
因此，如何将大模型的语义优势落地为低层可控、可验证的移动与操作策略，并在扰动与不确定条件下保持鲁棒，是当前研究的重要方向。

% 在具身智能的体系中，面向真实场景的通用移动操作尤为依赖高层语义理解与低层连续控制之间的紧密耦合。
% 一方面，视觉—语言导航（Vision-and-Language Navigation, VLN）等方向已经围绕“从自然语言到可达目标”的路径规划积累了丰富的任务设定与基准数据集，为机器人在复杂语义空间中进行主动探索与决策提供了重要支撑
% \cite{Zhang2024VLNFM}；
% 同时，无论是基于模仿学习(Imitation Learning, IL)还是离线强化学习(Offline Reinforcement Learning, RL)的操作方法，在对象识别、姿态估计、抓取规划等方面都取得了显著的进展，
% 并在真实机器人平台上得到了验证。
% 另一方面，以视觉—语言—动作（Vision-Language-Action, VLA）模型为代表的多模态基础模型开始尝试在统一框架下对图像、语言与动作轨迹进行联合建模，使得从开放式自然语言指令出发，
% 直接生成可执行的动作序列成为可能，并在具身智能综述中被视为连接高层语义意图与低层控制策略的关键环节
% \cite{Ma2024Survey}。
% 已有综述工作表明，相对于传统依赖手工特征与分阶段管线的移动操作系统，基于大模型的具身智能范式在任务泛化、跨环境迁移以及复杂指令理解方面具有明显潜力，但同时也暴露出如何兼顾安全性、可解释性与可验证性的新的挑战

从国家战略和社会发展需求的角度，明确提出要推动人工智能与实体经济深度融合，发展面向制造业、服务业、医疗和养老等领域的智能机器人与智能装备。
《新一代人工智能发展规划》提出：到2030年人工智能理论、技术与应用总体达到世界领先水平，成为世界主要人工智能创新中心，智能经济、智能社会取得明显成效，为跻身创新型国家前列和经济强国奠定重要基础
\cite{SC2017AIDevelopmentPlan}；
《“十四五”机器人产业发展规划》中进一步聚焦重点推进工业机器人、服务机器人、特种机器人重点产品的研制及应用，拓展机器人产品系列，提升性能、质量和安全性，推动产品高端化智能化发展。
\cite{MIIT2021RobotPlan}；
《高等学校人工智能创新行动计划》中强调要“瞄准世界科技前沿，不断提高人工智能领域科技创新、人才培养和国际合作交流等能力”
\cite{MOE2018AIActionPlan}。

在上述国际学术发展趋势和国家战略需求的双重驱动下，本文所聚焦的“面向通用场景的基于视觉—语言—动作模型的具身移动操作方法”，可以被视为顺应技术演进与应用需求的一项自然延伸与深化：
一方面，前沿综述表明，具身智能、移动操作与基础模型三者的交汇已成为当前机器人学的重要发展方向，有必要在统一视角下系统梳理从感知、表示到决策、控制的完整链条，进而形成面向真实任务的通用方法论框架。
另一方面，针对家庭、实验室、仓储等典型室内场景，现有系统在统一建模自然语言指令、环境语义信息与移动/操作决策方面仍存在明显局限，尚缺乏在保障安全性与可控性的前提下，兼顾任务多样性与环境变化的通用技术路线。
在此背景下，本课题的研究问题可以被较为清晰地界定为：如何利用视觉—语言—动作模型，在复杂室内环境中统一处理指令理解、环境感知与具身移动操作决策，从而支撑一体化的任务执行流程：
既对接具身智能和机器人学习在基础模型时代的发展脉络，又紧扣通用场景下典型具身移动操作任务的客观需求，为后续研究方案与技术路线的设计提供了问题基础与方向锚点。

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/overview.pdf}
  \caption{具身智能体整体框架示意图}
  \label{fig:overview}
\end{figure}

\section{研究意义}

从学术研究角度看，围绕“面向通用场景的基于视觉-语言-动作模型的具身移动操作方法”开展系统研究，有望在多个层面形成具有前瞻性的理论与方法贡献。
其一，通过引入统一的中间表示，将自然语言指令、环境语义信息与可执行动作在同一模型中进行对齐，使高层语义规划与低层几何/动力学控制之间的映射关系更加明确，
为移动与操作的协同规划提供可解释、可分析的语义—几何桥接机制
\cite{Anderson2018VLNR2R}；
其二，在 VLA 模型框架下，将指令理解、情境推理与分层技能序列生成整合为端到端的“语言—感知—动作”链路，有助于深化我们对基础模型在具身场景中“世界知识+具身经验”混合表征与推理能力的理解，
并为构建具有多任务泛化能力的通用具身智能体提供可行路径
\cite{Ahn2022SayCan,Brohan2022RT1,Driess2023PALME,Kim2024OpenVLA}；
其三，面向典型通用场景，对移动与操作强耦合带来的观测可达性、操作可行性与安全约束等问题进行系统刻画，并在统一评测框架下分析任务成功率、鲁棒性与泛化能力，
有望推动移动操作从“单场景工程系统”走向“可比较、可复现的研究基准”，为后续更大规模、更复杂场景下的具身移动操作研究提供方法与系统基础
\cite{Thakar2023Survey}。

从应用需求角度看，本课题的研究有望为实验室助理机器人、仓储物流机器人以及家居服务机器人等典型应用场景中的具身移动操作任务提供更加通用、可落地的技术方案。
通过在统一的视觉-语言-动作模型框架下提升机器人在通用场景中的整体能力，可以减少对环境的精细改造与大量人工规则设计，降低系统部署和维护成本，
并提升在任务变化、物体变化和环境部分改造情况下的持续可用性与可扩展性。
与此同时，得益于基础模型在多模态表征和知识迁移方面的优势，基于本课题形成的方法体系在扩展到新任务、新物体或新环境时，有望通过少量示例或在线交互快速适配，
进一步支撑具身智能系统在真实应用中的长期演进与迭代
\cite{Jiang2023VIMA,Shridhar2022CLIPORT,Shridhar2023PERACT,Zitkovich2023RT2}。

