% !TeX root = ../thuthesis-example.tex
\thusetup{
  cite-style = inline,
}

\chapter{国内外研究现状及分析}

% \section{具身导航与通用室内场景理解方法}

% 近年来，面向落地应用的导航技术经历了从传统移动机器人导航到具身导航（Embodied Navigation）的持续演进。传统移动机器人导航在建图、定位与路径规划等模块上已经形成了较为成熟的理论和工程体系
% \citep{Yao2023MobileNavCN,Abdulsaheb2023Classical}，
% 在室内服务机器人、AGV 等场景中广泛应用，但其多以几何可达性和路径最优性为核心目标，对语义理解和复杂交互的支持有限，难以直接支撑指令驱动的移动操作。
% 随着具身智能与大规模多模态模型的发展，研究逐渐转向“在未知或半未知环境中一边感知、一边交互、一边优化导航策略”的具身导航范式，强调利用本体传感器主动探索、在线构建语义地图与对象级表示
% \citep{Wu2024EmbodiedNavSurvey,Wang2025LLMEmbodiedSysCN,Gao2025EMLM}。
% 在感知模态上，出现了以纯视觉为主的具身视觉导航任务（Embodied Visual Navigation, E-VN），如 PointNav、ImageNav 和 ObjectNav 等
% \citep{Krantz2022InstanceImageNav,Chaplot2020SemExp}，
% 以及进一步引入语言指令约束的视觉–语言导航（Vision-and-Language Navigation, VLN），使机器人能够在仿真和真实环境中根据自然语言描述执行跨房间导航与目标搜索
% \citep{Anderson2018VLNR2R,Gu2022VLNSurvey}，
% 依托 Habitat 等高保真平台开展系统评测\citep{Savva2019Habitat}，在纯 RGB 或 RGB-D 输入下实现了从“到坐标”到“到物体/到语义位置”的能力提升。
% 与此同时，针对真实复杂环境中单一传感器易受遮挡和噪声影响的问题，大量工作探索了激光雷达、相机、毫米波雷达和 IMU 等多源传感器融合方案，
% 通过深度学习与概率图模型对多模态信息进行联合建模，以提高定位与环境感知的精度和鲁棒性
% \citep{Huang2022MultiModalSensorFusion,Zhang2020MultiModalSensorFusion}

% 在通用室内场景理解方面，研究重点从单帧 2D 语义分割逐步扩展到面向机器人应用的三维语义建图和场景级理解。
% 早期工作如 SemanticFusion 将卷积神经网络的语义预测与稠密 RGB-D SLAM 融合，在在线重建的三维网格上实现了实时语义标注
% \citep{McCormac2017SemanticFusion}；
% 随后，PanopticFusion 等系统进一步在体素地图中统一建模“背景类别”（stuff）与“前景物体”（things），支持大规模室内环境的全景语义映射与网格导出
% \citep{Narita2019PanopticFusion}。
% 在数据层面，ScanNet、Matterport3D 等大规模 RGB-D 室内数据集提供了丰富的三维重建与语义标注，为三维场景语义分割、实例分割和场景补全等任务奠定了基准
% \citep{Dai2017ScanNet,Chang2017Matterport3D}。
% 近期工作则开始关注开放词汇与高层关系建模：OpenScene、UniM-OV3D 等方法将图文预训练模型与三维特征对齐，实现对任意文本类别的开放词汇三维语义理解
% \citep{Peng2023OpenScene,He2024UniMOV3D}；
% Hierarchical Open-Vocabulary 3D Scene Graphs 等工作进一步构建开放词汇三维场景图，将物体、功能区域及其语义关系编码为结构化表示，为语言约束的导航与操作提供了可查询的场景先验
% \citep{Werby2024HierarchicalOV3DSG}。

% 在产业界，以自动驾驶为代表的车企和出行公司则在大规模道路数据和强算力平台支撑下，形成了两条具有代表性的落地路线：
% 一类以多传感器冗余为特点（如摄像头 + 激光雷达 + 高精地图）的模块化感知–规划–控制栈，强调安全冗余和可解释性；
% 另一类则以特斯拉 FSD v12 及后续版本为代表，采用“从像素到控制”的端到端视频 Transformer，仅依赖多路摄像头进行感知与决策，
% 在真实道路上大规模部署并通过在线数据闭环持续迭代
% \citep{Waymo2023TechBlog,Tesla2023FSDv12}。

% 从传统导航到具身导航、从几何建图到语义/全景语义映射、从封闭类别到开放词汇三维场景图，现有研究已经为具身移动操作提供了较为坚实的技术基础。
% 然而，这些工作多将导航与场景理解作为相对独立的感知与决策模块，评价指标也主要聚焦于到达率、定位精度或语义分割精度，
% 对“导航位置与视角选择如何影响后续操作可达性与安全性”、以及“如何在统一的视觉–语言–动作表示下将通用室内场景理解直接服务于移动操作决策”的系统研究仍然相对不足，
% 这也为后续基于 VLA 大模型构建一体化具身移动操作方法留出了重要空间。

\section{具身导航与通用室内场景理解方法}

近年来，面向实际应用的导航技术经历了从传统移动机器人导航到具身导航（Embodied Navigation）的持续演进。传统的移动机器人导航，在建图、定位与路径规划等模块上已经形成了较为成熟的理论和工程体系
\citep{Yao2023MobileNavCN,Abdulsaheb2023Classical}，
并在室内服务机器人、自动导引车（AGV）等场景中得到了广泛应用。
然而，传统导航方法主要聚焦于几何可达性和路径最优性，虽然在简单环境下效果显著，但在面对复杂语义场景和动态交互时，往往对环境理解和复杂任务的支持有限，难以直接支撑指令驱动的移动操作。

随着具身智能和大规模多模态模型的快速发展，研究逐渐转向“在未知或半未知环境中一边感知、一边交互、一边优化导航策略”的具身导航新范式。该范式强调通过本体传感器主动探索环境，
在线构建语义地图与对象级表示，从而使得机器人能够在动态、复杂的环境中进行更加智能的决策和执行
\citep{Wu2024EmbodiedNavSurvey,Wang2025LLMEmbodiedSysCN,Gao2025EMLM}。
在感知模态方面，具身视觉导航（Embodied Visual Navigation, E-VN）任务如PointNav、ImageNav和ObjectNav等，已逐渐成为研究重点
\citep{Krantz2022InstanceImageNav,Chaplot2020SemExp}。
这些任务主要依赖纯视觉输入，旨在提升机器人在无先验地图条件下的导航能力。
同时，视觉–语言导航（Vision-and-Language Navigation, VLN）的出现进一步拓展了具身导航的任务范围，使得机器人能够在仿真和真实环境中根据自然语言描述进行跨房间导航和目标搜索
\citep{Anderson2018VLNR2R,Gu2022VLNSurvey}，
并依托高保真平台如Habitat开展了广泛的系统评测\citep{Savva2019Habitat}，显著提升了从“到坐标”到“到物体/到语义位置”的导航能力。
与此同时，针对复杂环境中单一传感器易受遮挡和噪声干扰的问题，研究者们提出了多源传感器融合方法，通过将激光雷达、相机、毫米波雷达和惯性测量单元（IMU）等传感器的数据进行联合建模，
从而提高导航系统的精度和鲁棒性\citep{Huang2022MultiModalSensorFusion,Zhang2020MultiModalSensorFusion}。
这些多模态传感器的融合为具身导航提供了更为强大的环境感知能力，使得机器人能够在复杂、动态环境下实现精确的定位与安全的导航。

在通用室内场景理解方面，研究重点逐步从单帧2D语义分割扩展到面向机器人应用的三维语义建图与场景级理解。
早期的工作如SemanticFusion将卷积神经网络（CNN）用于语义预测，并结合稠密RGB-D SLAM实现了实时语义标注，进而在三维网格上实现了在线语义标注
\citep{McCormac2017SemanticFusion}；
Panoptic Fusion等系统进一步在体素地图中统一建模“背景类别”（stuff）与“前景物体”（things），支持大规模室内环境的全景语义映射与网格导出
\citep{Narita2019PanopticFusion}。
与此同时，随着大规模RGB-D数据集的出现，如ScanNet和Matterport3D等提供了丰富的三维重建与语义标注数据，为三维场景的语义分割、实例分割和场景补全等任务奠定了基准
\citep{Dai2017ScanNet,Chang2017Matterport3D}。
近年来的研究开始关注开放词汇与高层关系建模：OpenScene、UniM-OV3D等方法通过图文预训练模型与三维特征的对齐，实现了对任意文本类别的开放词汇三维语义理解
\citep{Peng2023OpenScene,He2024UniMOV3D}；
Hierarchical Open-Vocabulary 3D Scene Graphs等工作则进一步构建了开放词汇三维场景图，将物体、功能区域及其语义关系编码为结构化表示，为语言约束的导航与操作提供了可查询的场景先验
\citep{Werby2024HierarchicalOV3DSG}。

在产业界，以自动驾驶为代表的车企和出行公司在大规模道路数据和强算力平台的支撑下，已经形成了两条具有代表性的落地路线。
一类是以多传感器冗余为特点（如摄像头 + 激光雷达 + 高精度地图）的模块化感知–规划–控制栈，强调安全冗余与可解释性；
另一类则以特斯拉FSD v12及后续版本为代表，采用“从像素到控制”的端到端视频Transformer，仅依赖多路摄像头进行感知与决策，并已在真实道路上大规模部署，依靠在线数据闭环持续迭代
\citep{Waymo2023TechBlog,Tesla2023FSDv12}。

从传统导航到具身导航、从几何建图到语义/全景语义映射、从封闭类别到开放词汇三维场景图，现有研究为具身移动操作提供了坚实的技术基础。
然而，这些工作大多将导航与场景理解作为独立的感知与决策模块，评价指标也主要聚焦于到达率、定位精度或语义分割精度等，
对于“导航位置与视角选择如何影响后续操作的可达性与安全性”以及“如何在统一的视觉–语言–动作表示下，将通用室内场景理解直接服务于移动操作决策”的系统研究仍然存在不足，
这为后续基于VLA大模型构建一体化具身移动操作方法留下研究空间。


% \section{基于学习的具身操作与移动操作控制方法}

% 在具身操作（Embodied Manipulation）方面，近年来的控制方法大致经历了从“感知–规划–控制”模块化管线向端到端学习范式的转变，并围绕模仿学习（IL）、强化学习（RL）及其融合展开系统研究
% \citep{Han2023DRLManip,Celemin2022IILSurvey}。
% 早期代表性工作通过引入导引策略搜索（Guided Policy Search）等方法，直接从相机图像端到端学习将视觉映射到关节力矩或末端位姿的深度视觉运动策略，显著简化了传统操作系统中手工设计特征与控制器的流程
% \citep{Levine2016EndToEndVisuomotor}；
% 在此基础上，大规模深度强化学习框架（如 QT-Opt）利用数十万次真实抓取试验离线估计 Q 函数，实现了在单摄像头输入下对未知物体的高成功率闭环抓取，并自动涌现试探、重抓、物体调整等复杂行为
% \citep{Kalashnikov2018Scalable}。
% 与此同时，模仿学习方向从传统行为克隆扩展到语言条件与多任务设定，如 BC-Z 通过统一处理多任务、多模态演示，在相同策略中实现了零样本任务泛化与语言条件操作，从而在一定程度上缓解了“单任务单策略”的局限
% \citep{Jang2022BCZ}。
% 为了更好刻画多模态动作分布并提高复杂任务的稳定性，扩散策略（Diffusion Policy）提出将机器人视觉运动策略表示为条件去噪扩散过程，在多项基准任务上相较于传统 IL/RL 方法取得显著性能提升
% \citep{Chi2025DiffusionPolicy}，
% 后续 3D Diffusion Policy（DP3）进一步将稀疏点云编码为紧凑三维表示，显著提升了在空间、视角和外观变化下的泛化能力
% \citep{Ze20243DPolicy}。

% 在移动操作（Mobile Manipulation）控制方法方面，传统系统多采用底盘导航与机械臂操作解耦的架构：由导航模块规划机器人基座的移动轨迹，到达目标附近后再调用抓取或操作控制器完成局部交互，
% 这一分而治之的设计便于工程集成与安全验证，但在需要频繁“边走边动”、对位姿耦合要求高的任务（如推门、从狭窄空间中取物）中容易出现协同不充分的问题。
% 近年来，一批端到端或分层一体化的移动操作控制方法开始涌现：
% Skill Transformer 在统一 Transformer 框架下同时预测高层技能（如导航、抓取、放置）和全身低层动作，实现了在长时序重排任务中对移动与操作的联合建模，相比传统分阶段方法显著减少了中间切换带来的误差累积
% \citep{Huang2023SkillTransformer}；
% Deep Whole-Body Control 则通过深度强化学习统一优化导航与操作，使机器人在仅使用 RGB 视觉的条件下即可在仿真和真实公寓中完成多种日常移动操作任务，验证了端到端策略在复杂家庭环境中的可行性
% \citep{Fu2023Deep}。
% 在数据驱动的移动操作模仿学习方面，Mobile ALOHA 提出了低成本全身远程示教平台，通过采集包含底盘、双臂和手部在内的整机演示轨迹，并与已有桌面操作数据联合训练行为克隆策略，
% 在开关柜门、使用电梯、烹饪等长时序移动操作任务上取得了显著性能提升
% \citep{Fu2024MobileAloha}。

% 现有具身操作与移动操作方法从端到端 IL/RL、扩散策略到技能级分层控制均取得了丰富进展，为机器人在复杂环境中完成操作任务提供了多种可选技术路线；
% 但多数工作仍聚焦于单机位桌面场景或特定环境下的任务集，对“通用室内场景中基于语言指令的具身移动操作”这一目标而言，
% 在跨场景泛化、移动–操作协同决策以及与高层语义（如 VLA 模型）紧耦合等方面仍存在较大提升空间。

\section{基于学习的具身操作与移动操作控制方法}

在具身操作（Embodied Manipulation）领域，近年来的控制方法大致经历了从“感知–规划–控制”模块化管线向端到端学习范式的转变，
研究围绕模仿学习（Imitation Learning, IL）、强化学习（Reinforcement Learning, RL）及其融合展开了大量系统性研究
\citep{Han2023DRLManip,Celemin2022IILSurvey}。
早期的代表性工作通过引入导引策略搜索（Guided Policy Search）等方法，直接从相机图像端到端学习将视觉映射到关节力矩或末端位姿的深度视觉运动策略，
显著简化了传统操作系统中手工设计特征与控制器的流程
\citep{Levine2016EndToEndVisuomotor}；
这些方法通过强化学习对系统进行训练，逐步实现了对多样化操作任务的有效支持。
在此基础上，基于大规模深度强化学习框架（如 QT-Opt），利用数十万次真实抓取试验离线估计 Q 函数，在单摄像头输入下实现了对未知物体的高成功率闭环抓取，并自动涌现了试探、重抓、物体调整等复杂行为
\citep{Kalashnikov2018Scalable}。
此外，模仿学习方向逐渐从传统行为克隆（Behavior Cloning, BC）扩展到语言条件与多任务设定，如 BC-Z 通过统一处理多任务、多模态演示，在同一策略中实现了零样本任务泛化与语言条件操作，
从而在一定程度上缓解了“单任务单策略”的局限性
\citep{Jang2022BCZ}。
为了解决在复杂任务中的稳定性和多模态动作分布问题，扩散策略（Diffusion Policy）应运而生，它提出将机器人视觉运动策略表示为条件去噪扩散过程，
从而使得机器人能够在复杂环境下生成更加稳定且具有鲁棒性的动作序列。该方法在多项基准任务上相较于传统的 IL/RL 方法取得了显著性能提升
\citep{Chi2025DiffusionPolicy}。
进一步的，3D Diffusion Policy（DP3）通过将稀疏点云编码为紧凑的三维表示，显著提升了在空间、视角和外观变化下的泛化能力
\citep{Ze20243DPolicy}，为复杂场景下的操作任务提供了强有力的支持。

在移动操作（Mobile Manipulation）控制方法方面，传统系统普遍采用底盘导航与机械臂操作解耦的架构：导航模块规划机器人基座的移动轨迹，在到达目标附近后再调用抓取或操作控制器完成局部交互。
这种分而治之的设计便于工程集成与安全验证，但在需要频繁“边走边动”、对位姿耦合要求高的任务（如推门、从狭窄空间中取物）中，容易出现协同不充分的问题
。为了解决这一问题，近年来一批端到端或分层一体化的移动操作控制方法开始涌现。
例如，Skill Transformer 在统一的 Transformer 框架下同时预测高层技能（如导航、抓取、放置）和全身低层动作，实现了在长时序重排任务中的移动与操作联合建模，
相比传统的分阶段方法显著减少了中间切换带来的误差累积
\citep{Huang2023SkillTransformer}；
Deep Whole-Body Control通过深度强化学习统一优化导航与操作，使机器人能够在仅使用 RGB 视觉的条件下，在仿真和真实环境中完成多种日常移动操作任务，验证了端到端策略在复杂家庭环境中的可行性
\citep{Fu2023Deep}。
在数据驱动的移动操作模仿学习方面，Mobile ALOHA 提出了低成本全身远程示教平台，通过采集包含底盘、双臂和手部在内的整机演示轨迹，并与已有桌面操作数据联合训练行为克隆策略，
在开关柜门、使用电梯、烹饪等长时序移动操作任务上取得了显著性能提升\citep{Fu2024MobileAloha}。
这些工作通过利用多源数据和深度学习技术，有效解决了长时序操作任务中的时序依赖和控制精度问题。

目前，具身操作与移动操作方法在端到端IL/RL、扩散策略到技能级分层控制等方面都取得了丰富进展，为机器人在复杂环境中完成操作任务提供了多种可选的技术路线。
然而，现有研究多聚焦于单机位桌面场景或特定环境下的任务集，尚未充分考虑如何将这些方法应用于“通用室内场景中基于语言指令的具身移动操作”这一目标。
具体来说，跨场景泛化、移动—操作协同决策以及与高层语义（如视觉—语言—动作模型）紧密耦合等方面仍然存在较大的提升空间。



% \section{大模型驱动的具身移动操作策略}

% 近年来，大模型驱动的具身策略逐渐成为连接高层语义理解与低层控制的重要方向，其技术路径大致可分为：“LLM+技能库规划”、“端到端视觉–语言–动作（VLA）策略”和“跨形态通用策略与系统落地”三类。
% 以 PaLM-SayCan 为代表的工作首先将大型语言模型与价值函数/技能库相结合，通过语言模型进行任务分解与技能排序，再由预定义的导航与操作控制器执行，
% 实现在真实环境中从自然语言到多步机器人操作的闭环，并证明更强大语言模型可以显著提升规划正确率与任务成功率
% \citep{Ahn2022SayCan}。
% Code as Policies 等方法进一步利用代码生成类 LLM，将自然语言指令直接翻译为可执行的策略代码，通过程序结构表达感知处理与控制 API 组合，实现可解释、可编辑的语言到策略映射
% \citep{Liang2022CodeAsPolicies}，为大模型参与具身规划提供了更灵活的中间表示。
% 在端到端 VLA 方向，RT-1 首次系统性地将 Transformer 架构应用于从多任务真实机器人轨迹中学习“图像/文本到离散动作 token”的策略，在大规模厨房操作数据上展现出良好的任务泛化能力
% \citep{Brohan2022RT1}；
% 其后提出的 RT-2 将互联网尺度视觉–语言预训练与机器人数据结合，将 VLM 扩展为 VLA，使单一模型既具备 web 语义知识又能输出可执行机器人动作，在零样本泛化与语义推理方面取得显著提升
% \citep{Brohan2023RT2}。
% Open X-Embodiment 数据集与 RT-X 模型进一步整合了来自 22 种机器人形态、百万余真实轨迹的数据，表明在大规模跨平台数据上训练的高容量模型可以在多种机器人之间实现正迁移
% \citep{ONeill2024OpenX}。
% 在开放社区方面，OpenVLA 在 Open X-Embodiment 等数据上预训练 7B 级 VLA 模型，提供了统一的“视觉编码器 + 语言主干 + 动作头”架构及可复用代码库，使研究者能够在多种机械臂与任务上进行高效微调与迁移
% \citep{Kim2024OpenVLA}；
% Physical Intelligence 提出的 $\pi_{0}$ 将图像、文本与动作统一为连续流模型，在多机器人、多场景上表现出较好的泛化能力，被视为“机器人基础模型”的代表之一
% \citep{PI2024Pi0}。
% 同时，最新的综述工作从动作 token 化、分层结构和训练范式的角度，对现有 VLA 模型的架构、数据与应用进行了系统梳理，指出不同模型在开放词汇理解、长时序推理与低层精细控制上的优势与不足
% \citep{Ma2024VLASurvey,Zhong2025VLASurvey}。

% 面向具身移动操作，部分工作开始关注如何将固定基座操作 VLA 模型迁移到“可移动底盘 + 机械臂”的场景。
% MoManipVLA 提出在不重新收集大规模移动操作数据的前提下，利用预训练 VLA 模型预测高泛化能力的末端执行器 waypoint，通过双层优化联合规划底盘与机械臂轨迹，在 OVMM 基准和真实环境中显著提升了跨任务、跨环境的成功率
% \citep{Wu2025MoManipVLA}。
% 工业界方面，Gemini Robotics 1.5 等系统在通用多模态大模型基础上引入动作通道和具身推理模块，使机器人能够通过“推理–规划–执行”的流程，从视觉与语言输入生成多平台可迁移的控制命令，
% 并进一步推出可在机器人本体上本地运行的轻量版本，以降低时延与对网络的依赖
% \citep{Abdolmaleki2025GeminiRobotics}。
% 总体来看，大模型驱动的具身策略在统一建模视觉、语言与动作、提升跨任务与跨场景泛化方面已经取得了显著进展，但现有系统多聚焦于固定基座操作或局部场景，
% 对“通用室内场景中导航与精细操作强耦合”的具身移动操作任务，尚缺乏同时兼顾语义推理、几何可达性、安全约束与实时性的统一方法框架，这也为本课题基于 VLA 构建具身移动操作方法留下了明显的研究空间。

\section{大模型驱动的具身移动操作策略}

近年来，大模型驱动的具身策略逐渐成为连接高层语义理解与低层控制的关键技术方向。
随着基础模型（Foundation Models）在自然语言处理和计算机视觉等领域的突破，其在具身智能中的应用，特别是在移动操作和任务执行中的潜力，也得到了广泛关注。
其技术路径大致可分为以下几类：“LLM+技能库规划”、“端到端视觉–语言–动作（VLA）策略”和“跨形态通用策略与系统落地”。

在“LLM+技能库规划”方向，以 PaLM-SayCan 为代表的工作，首次将大型语言模型（LLM）与价值函数或技能库相结合，通过语言模型进行任务分解与技能排序，再由预定义的导航与操作控制器执行，
实现了从自然语言到多步机器人操作的闭环\citep{Ahn2022SayCan}。这
一方法不仅证明了语言模型的强大能力可以显著提升规划正确率和任务成功率，同时也为具身智能任务的语义推理和任务规划提供了新的思路。
Code as Policies 等方法进一步利用代码生成类 LLM，将自然语言指令直接翻译为可执行的策略代码，程序结构可以直接表达感知处理与控制 API 的组合，显著提升了大模型参与具身规划的灵活性和可解释性
\citep{Liang2022CodeAsPolicies}。
在端到端 VLA（视觉-语言-动作）方向，RT-1 首次系统性地将 Transformer 架构应用于从多任务真实机器人轨迹中学习“图像/文本到离散动作 token”的策略，在大规模厨房操作数据上展现了良好的任务泛化能力
\citep{Brohan2022RT1}；
其后提出的 RT-2 将互联网规模的视觉–语言预训练与机器人数据结合，将视觉-语言模型（VLM）扩展为 VLA，使单一模型既具备 web 语义知识又能输出可执行的机器人动作，
在零样本泛化与语义推理方面取得了显著提升
\citep{Zitkovich2023RT2}。这一研究表明，基于大模型的具身操作方法能够在更广泛的任务范围内实现较好的推理能力和泛化能力，尤其是在动态环境下的适应性显著增强。
Open X-Embodiment 数据集与 RT-X 模型进一步整合了来自22种机器人形态、百万余真实轨迹的数据，表明在大规模跨平台数据上训练的高容量模型可以在多种机器人平台之间实现正迁移
\citep{ONeill2024OpenXEmbodiment}。
这类研究拓展了具身智能领域的跨平台应用能力，使得大模型不仅限于特定硬件平台，也能够在不同平台上进行广泛部署和灵活适配。
在开放社区方面，OpenVLA 在 Open X-Embodiment 等数据集上预训练了 7B 级 VLA 模型，提供了统一的“视觉编码器 + 语言主干 + 动作头”架构及可复用代码库，
使研究者能够在多种机械臂与任务上进行高效微调与迁移
\citep{Kim2024OpenVLA}；
此外，Physical Intelligence 提出的$\pi_{0.5}$模型将图像、文本与动作统一为连续流模型，在多机器人、多场景的测试中表现出了较好的泛化能力，成为“机器人基础模型”的代表之一
\citep{Intelligence2025pi_05}。
尽管在多个领域取得了显著进展，现有的VLA大模型在具身操作中的应用仍面临诸多挑战。
首先，模型在开放词汇理解和长时序推理方面的能力仍然有限，尤其是需要在长时间跨度的任务中进行推理与决策时，模型的精度和稳定性仍需要进一步提升
\citep{Ma2024Survey}；
其次，虽然VLA模型在语义推理和任务规划方面表现出色，但在低层精细控制与实际操作过程中如何保持高效性与鲁棒性，仍然是一个重要的研究方向
\citep{Zhong2025Survey}。

在具身移动操作的应用中，部分工作开始关注如何将固定基座操作 VLA 模型迁移到“可移动底盘 + 机械臂”的场景中。
例如，MoManipVLA 提出了在不重新收集大规模移动操作数据的前提下，利用预训练的 VLA 模型预测高泛化能力的末端执行器 waypoint，并通过双层优化联合规划底盘与机械臂轨迹，
显著提升了跨任务、跨环境的成功率\citep{Wu2025MoManipVLA}。
这一方法使得具身操作能够在更加复杂和动态的环境中进行高效执行，且不需要重新收集大量的训练数据。
在工业界，Gemini Robotics 1.5 等系统通过引入动作通道和具身推理模块，使机器人能够通过“推理–规划–执行”的流程，从视觉与语言输入生成多平台可迁移的控制命令。
此类系统进一步推出了轻量版的本地运行版本，以降低时延与对网络的依赖，从而提升了在现实环境中的应用效率\citep{Team2025GeminiRobotics}。
这些系统的成功应用证明了大模型驱动的具身策略不仅能提供较高的任务执行能力，还能解决网络依赖和实时性的问题。

大模型驱动的具身策略在统一建模视觉、语言与动作、提升跨任务与跨场景泛化方面已经取得了显著进展，尤其是在大规模数据集和高计算平台的支持下，模型的学习能力和应用范围不断拓展。
然而，现有系统仍然多聚焦于固定基座操作或特定场景下的任务集，对“通用室内场景中导航与精细操作强耦合”的具身移动操作任务，尚缺乏能够同时兼顾语义推理、几何可达性、安全约束与实时性的统一方法框架。
基于 VLA 的具身移动操作方法有望通过整合视觉感知、语言理解与运动控制，克服当前模型在任务迁移、跨平台部署以及多场景适应等方面的局限，为更复杂的具身智能任务提供强有力的支持。


\section{开放场景基准与移动操作系统集成框架}

在具身智能逐渐走向落地应用的过程中，开放场景基准与系统集成框架发挥了承上启下的关键作用：
一方面，系统化的基准任务与评价指标为不同算法提供了可比较的实验环境与统一问题定义；
另一方面，面向真实机器人系统的集成框架则将感知、导航与操作串联为可部署的工程流水线，为从仿真到真机的迁移提供了基础支撑。
相关研究大致沿着“从桌面单点操作到多任务具身操作基准”、“从局部场景到全屋尺度开放场景移动操作基准”和“从仿真环境到端到端移动操作系统集成”的脉络逐步演进。

在基准任务层面，早期工作主要聚焦于机械臂单点操作与多任务强化学习评测。
RLBench 提出包含近百个操作任务的标准基准与仿真环境，覆盖抓取、插拔、开关等多种典型操作，为基于视觉的深度强化学习与模仿学习算法提供了统一评测平台
\citep{James2020RLBench}。
Meta-World 则面向多任务与元强化学习，定义了 50 个 MuJoCo 操作任务，并给出跨任务泛化与快速适应能力的系统评估方案
\citep{Yu2020MetaWorld}。
在此基础上，ManiSkill2 将任务规模扩展到包含刚体、软体、关节体等多类对象的 20 余种技能族，并显式区分“固定基座操作”和“移动基座操作”，同时提供大规模演示数据与高效视觉强化学习接口，
为算法在多模态输入、任务多样性与仿真效率之间的折中提供了优良平台
\citep{Gu2023ManiSkill2}。
这类基准多以桌面操作或局部场景为主，对环境尺度和语义开放性要求相对有限，但在统一任务接口、动作空间和评测指标方面奠定了重要基础。
随着语言条件与长时序需求的提升，一批面向“语言–操作”一体化的具身基准开始出现。CALVIN 基准在逼真的模拟场景中定义了多阶段、长时序的语言条件操作任务，
强调在视觉遮挡、物体重排等扰动下的策略鲁棒性与重规划能力
\citep{Mees2022CALVIN}；
LIBERO 系列基准则通过构造多套家居场景中的语言条件操作任务，系统分析了知识迁移与终身学习在机器人操作中的可行性与挑战，为跨任务迁移和持续学习算法提供了标准化测试平台
\citep{Liu2023Libero}。
近期的 VLABench 等工作在此基础上进一步扩大任务规模与语言表达范围，强调开放词汇指令、长时序逻辑约束和 VLA 模型评测，为大模型驱动的具身操作策略提供了系统化对比基准
\citep{Zhang2025VLBench}。
这一类基准在动作粒度、任务复杂度和语言表达丰富性方面不断提升，但大多仍假设机械臂基座固定，对导航–操作强耦合的移动操作场景覆盖有限。
面向更贴近真实住宅与服务场景的需求，近年的一条重要发展路线是构建“全屋尺度、物体可交互、任务多样化”的开放场景具身基准。
Habitat 2.0 在 ReplicaCAD 等高保真室内模型基础上，提出了 Home Assistant Benchmark（HAB），定义“收拾房间、整理杂物、餐桌布置”等长时序移动操作任务，
并系统比较了分层强化学习与传统 Sense–Plan–Act 管线在此类任务上的性能与泛化能力
\citep{Szot2021Habitat}。
iGibson 2.0 则强调物体中心建模和物理交互逼真度，面向日常家务任务构建了多种移动操作场景，支持机器人在复杂布置环境中进行导航、开关门、搬运等操作，
为研究从视觉感知到物理推拉、抓取的闭环策略提供了可扩展环境
\citep{Li2021iGibson}。
BEHAVIOR 与后续的 BEHAVIOR-1K 进一步从“人类日常活动库”的角度出发，将数以千计的日常活动分解为一系列具身任务模板，
并在 OmniGibson 等高保真模拟器中实例化，覆盖从物体收纳、清洁到烹饪等多类家务活动，为人类中心具身智能与移动操作研究提供了前所未有的任务覆盖度
\citep{Li2023Behavior1K}。

针对移动操作这一具体方向，一些基准开始显式将底盘导航与机械臂操作纳入统一任务定义。
ManiSkill2 中的 Push Chair、Open Cabinet 等任务将移动基座与操作目标耦合在一起，要求策略在大范围探索中寻找合理操作位姿
\citep{Gu2023ManiSkill2}；
HomeRobot 项目提出的 Open-Vocabulary Mobile Manipulation（OVMM）基准，
在仿真与真实 Hello Robot Stretch 平台上统一定义了“语言指令 \textrightarrow{} 导航 \textrightarrow{} 感知 \textrightarrow{} 操作”的端到端移动操作链路，
强调开放词汇目标指定与跨环境泛化能力
\citep{Paxton2023HomeRobot}。
LaNMP 等新近基准则从多维度系统刻画移动操作难度，包括场景规模、视角变化、物体遮挡与语义多样性等，并提出细粒度的分阶段成功率与安全性指标，
推动了对“导航–操作–语义理解联动关系”的定量研究\citep{Jaafar2024LaNMP}。
这些工作在不同程度上体现出向“通用、开放场景移动操作”方向演化的趋势，但在大规模数据采集成本、真实场景复杂度与评测标准统一性方面仍面临不小挑战。

在系统集成与工程落地方面，主流开放平台也在逐步形成“仿真–算法–真机”一体的移动操作研究范式。
一方面，Habitat-Lab、OmniGibson、ManiSkill2 等框架在仿真端提供了可扩展场景资源、统一任务接口与多种控制模式，使研究者能够在同一平台内比较 IL、RL、VLA 等多种算法，
并支持与 ROS/ROS~2 等机器人中间件打通，简化了策略上板流程
\citep{Szot2021Habitat,Li2023Behavior1K,Gu2023ManiSkill2}。
另一方面，Robosuite、Isaac Gym 等面向机器人学习的仿真框架通过模块化任务定义与 GPU 加速物理仿真，显著降低了大规模策略训练与消融实验的门槛，
并为后续在 Isaac Sim 等工业级仿真环境及真实机器人上的迁移提供了接口
\citep{Zhu2020RoboSuite,Makoviychuk2021IsaacGym}。
以 HomeRobot 软件栈为代表的移动操作系统进一步在 LoCoBot、Stretch 等低成本平台上集成导航、抓取与开放词汇感知模块，形成了从仿真到真实家居环境的完整实验流程，
为研究者验证语言驱动移动操作策略提供了开源工程基础
\citep{Paxton2023HomeRobot}。

综合来看，现有开放场景基准与系统集成框架已经在任务多样性、场景真实性和算法可复现性等方面取得了显著进展，为大规模评测具身智能算法和推动移动操作落地提供了重要支撑。
然而，从“面向通用室内场景的、基于视觉–语言–动作模型的一体化具身移动操作方法”这一目标出发，仍然存在若干明显空缺：
一是多数基准在任务定义上仍偏向固定基座或局部场景，对导航–操作强耦合下的安全视角选择、主动感知与失败恢复等问题覆盖有限；
二是现有系统多将大模型作为高层规划或语义标注模块，缺乏在统一 VLA 表示下对导航策略与低层操作控制进行系统评测的基准与软件接口；
三是针对开放词汇目标、动态场景扰动与跨场景迁移的联合评测仍处于起步阶段。
围绕这些不足，本课题在后续研究中将结合前述开放基准与系统框架，进一步构建适用于通用室内场景的移动操作任务集与评测方案，并在此基础上设计和验证基于 VLA 模型的一体化具身移动操作方法。

\section{本章小结}

本章围绕具身移动操作的相关研究进展，对具身导航与通用室内场景理解、基于学习的具身操作与移动操作控制、大模型驱动的具身移动操作策略以及开放场景基准与系统集成框架等方向进行了系统梳理。
总体来看，现有工作在多个层面取得了显著成果：在感知与环境建模上，从传统几何建图逐步演进到语义映射、全景语义与开放词汇三维场景理解，为机器人在复杂室内环境中的稳健感知与语义理解奠定了坚实基础；
在控制方法上，从模块化“感知–规划–控制”到端到端 IL/RL 及扩散策略，再到一体化的全身移动操作控制，显著提升了机器人在长时序、多阶段操作任务中的学习效率与表现能力；
在大模型驱动的具身策略方面，以 VLA 为代表的模型初步展示了统一建模视觉、语言与动作、利用大规模跨平台数据实现跨任务与跨形态泛化的潜力；
在基准与系统层面，各类开放场景基准与仿真–真机一体化软件栈，推动了具身智能算法的可复现性和系统化评估。

然而，现有研究也表明，距离“面向通用室内场景、基于视觉–语言–动作模型的一体化具身移动操作”仍存在差距。
一方面，导航与操作在多数系统中仍然以弱耦合或后期集成的方式存在，对“导航视角选择如何影响操作可达性与安全性”“如何在移动–操作强耦合条件下实现主动感知与失败恢复”等关键问题缺乏系统刻画和统一建模；
另一方面，当前 VLA 模型多聚焦于固定基座或局部场景，在长时序推理、低层精细控制、实时性与安全约束等方面，尚未形成适用于移动操作任务的成熟方法框架，
与现有开放基准之间也缺少针对性强、接口友好的评测体系。
此外，面向开放词汇目标、动态环境扰动与跨场景迁移的联合评估体系仍然不完善，难以全面反映方法在真实通用场景中的有效性与鲁棒性。
基于上述成就与不足，可以看出：构建一种在统一视觉–语言–动作表示下，将通用室内场景理解、具身导航与精细操作紧密耦合的移动操作方法，并在开放基准与真实平台上予以验证，
既是延展现有研究脉络的必然方向，也是支撑服务机器人、实验室助理机器人和仓储物流机器人等应用落地的现实需求。

